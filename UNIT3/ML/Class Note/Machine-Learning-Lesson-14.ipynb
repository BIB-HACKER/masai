{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa4923b",
   "metadata": {},
   "source": [
    "# NLP with Bag of Words\n",
    "\n",
    "<img src=\"https://aiml.com/wp-content/uploads/2023/02/disadvantage-bow-1024x650.png\" height=500 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66582f9",
   "metadata": {},
   "source": [
    "Implementing NLP models involves a systematic approach that can be divided into several key steps. Each of these steps plays a crucial role in ensuring the success of the NLP project. Here, we delve into the description of each step, explaining their importance and how they contribute to the overall NLP model development process.\n",
    "\n",
    "#### 1. Data Collection\n",
    "\n",
    "**Description:** Data collection is the foundational step where you gather the raw material for your NLP project. The nature of your project determines the type of data you need. This could range from social media posts, news articles, and books, to transcripts of spoken language. The data can be collected through various means, including APIs, web scraping, public datasets, or proprietary sources.\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "- Determines the scope and potential of your NLP model.\n",
    "- Impacts the accuracy and reliability of the model outcomes.\n",
    "- Ensures diversity and representativeness in the dataset, which helps in building robust models.\n",
    "\n",
    "#### 2. Data Preprocessing\n",
    "\n",
    "**Description:** Data preprocessing involves cleaning and preparing your text data for modeling. This step may include several tasks such as tokenization (breaking text into tokens or words), removing stop words (common words that add little value), case normalization (converting all text to the same case), stemming and lemmatization (reducing words to their base or root form), and handling of missing or special characters.\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "- Improves model efficiency by eliminating irrelevant information.\n",
    "- Helps in standardizing the text data, making it more suitable for feature extraction and modeling.\n",
    "- Reduces computational complexity, leading to faster training times.\n",
    "\n",
    "#### 3. Feature Extraction\n",
    "\n",
    "**Description:** Feature extraction converts text into a numerical format that can be used by machine learning algorithms. Common techniques include Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and word embeddings (like Word2Vec or GloVe). BoW and TF-IDF focus on the frequency of words, whereas embeddings provide a dense representation capturing semantic relationships between words.\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "- Transforms text into a structured form that models can understand and learn from.\n",
    "- Captures important characteristics of the text, such as context or semantic meaning.\n",
    "- Determines the dimensionality of the input data, which can affect model performance and complexity.\n",
    "\n",
    "#### 4. Model Selection\n",
    "\n",
    "**Description:** Model selection involves choosing the appropriate algorithm or approach for your NLP task. This could range from traditional machine learning models like Logistic Regression, Naive Bayes, and Random Forests, etc. \n",
    "\n",
    "**Importance:**\n",
    "\n",
    "- Influences the accuracy, efficiency, and scalability of your NLP solution.\n",
    "- Depends on the nature of the task (e.g., classification, translation, generation) and the complexity of the data.\n",
    "- Affects the interpretability of the model and its outcomes.\n",
    "\n",
    "#### 5. Model Training\n",
    "\n",
    "**Description:** Model training is the process of feeding the preprocessed and vectorized text data into the chosen model, allowing it to learn from the data. This step involves setting parameters, choosing an optimizer, and defining a loss function. The model iteratively adjusts its weights based on the input data and the feedback from the loss function until it performs optimally.\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "- Directly affects the model's ability to make accurate predictions or generate coherent text.\n",
    "- Requires careful tuning of hyperparameters to balance between underfitting and overfitting.\n",
    "- Involves validation techniques such as cross-validation or split validation to gauge model performance on unseen data.\n",
    "\n",
    "#### 6. Evaluation and Tuning\n",
    "\n",
    "**Description:** After training, the model is evaluated using metrics appropriate to the specific NLP task, such as accuracy, precision, recall, F1 score for classification tasks, or BLEU score for translation tasks. Based on these metrics, further tuning of the model's parameters or architecture might be necessary to improve performance.\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "- Provides insight into how well the model performs on unseen data.\n",
    "- Helps identify biases, underfitting, or overfitting within the model.\n",
    "- Essential for optimizing the model to achieve the best balance between performance and generalization.\n",
    "\n",
    "\n",
    "Throughout these steps, collaboration between domain experts, data scientists, and engineers is crucial for addressing the challenges unique to NLP projects, from understanding the nuances of language to deploying scalable NLP solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4077a8",
   "metadata": {},
   "source": [
    "#### 1. Data Collection\n",
    "Data collection is the first step, where you gather the textual data you'll use for your model. This can involve scraping websites, using APIs, or accessing pre-existing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8410bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91766094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./cyberbullying-classification\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download('https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72e6e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cyberbullying-classification/cyberbullying_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53cd3e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47687</th>\n",
       "      <td>Black ppl aren't expected to do anything, depe...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47688</th>\n",
       "      <td>Turner did not withhold his disappointment. Tu...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47689</th>\n",
       "      <td>I swear to God. This dumb nigger bitch. I have...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47690</th>\n",
       "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGE...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47692 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type\n",
       "0      In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1      Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2      @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3      @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4      @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
       "...                                                  ...                ...\n",
       "47687  Black ppl aren't expected to do anything, depe...          ethnicity\n",
       "47688  Turner did not withhold his disappointment. Tu...          ethnicity\n",
       "47689  I swear to God. This dumb nigger bitch. I have...          ethnicity\n",
       "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGE...          ethnicity\n",
       "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...          ethnicity\n",
       "\n",
       "[47692 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "256fe34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "religion               7998\n",
       "age                    7992\n",
       "gender                 7973\n",
       "ethnicity              7961\n",
       "not_cyberbullying      7945\n",
       "other_cyberbullying    7823\n",
       "Name: cyberbullying_type, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cyberbullying_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb63a9",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing\n",
    "\n",
    "Preprocessing involves cleaning and preparing the text data for modeling. This may include tokenization, removing stop words, stemming, and lemmatization.\n",
    "\n",
    "**Example Code:** Tokenization, removing stop words & stemming using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b942c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samanvitha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samanvitha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'in',\n",
       " 'Hyderabad',\n",
       " '.',\n",
       " 'She',\n",
       " 'picked',\n",
       " 'me',\n",
       " 'up',\n",
       " 'from',\n",
       " 'the',\n",
       " 'airport',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example text\n",
    "text = \"I am in Hyderabad. She picked me up from the airport.\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aed0a112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5959ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalized_word is NOT in stop_words, can it be a meaningful word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e25ab5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am in Hyderabad. She picked me up from the airport.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2862c055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hyderabad', '.', 'picked', 'airport', '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "960b9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee9e2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = [SnowballStemmer(language='english').stem(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78000dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hyderabad', '.', 'pick', 'airport', '.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2c521",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "In this example, we first tokenize a sample sentence into words using NLTK's word_tokenize function. Next, we filter out stop words (common words like \"is\", \"an\", which are typically removed in NLP tasks) from the tokenized words and perform stemming. This preprocessing step is crucial for reducing the size of the dataset and focusing on the words that carry more meaning for analysis or modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22808139",
   "metadata": {},
   "source": [
    "#### 3. Feature Extraction\n",
    "Feature extraction involves converting text data into numerical formats that machine learning models can work with, such as using Bag of Words, TF-IDF, or word embeddings.\n",
    "\n",
    "**Example Code:** Creating a CountVectorizer with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "159ccbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Stopwords removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Stemming function\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    stemmed_tokens = stem_tokens(filtered_tokens, stemmer)\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Apply preprocessing to the tweet_text column\n",
    "df['preprocessed_text'] = df['tweet_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "01f36209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>word # katandandr , food crapilici ! # mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td># aussietv white ? # mkr # theblock # imaceleb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@ xochitlsuckkk classi whore ? red velvet cupc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@ jason_gio meh . : p thank head , concern ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>@ rudhoeenglish isi account pretend kurdish ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47687</th>\n",
       "      <td>Black ppl aren't expected to do anything, depe...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>black ppl n't expect anyth , depend anyth . ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47688</th>\n",
       "      <td>Turner did not withhold his disappointment. Tu...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>turner withhold disappoint . turner call court...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47689</th>\n",
       "      <td>I swear to God. This dumb nigger bitch. I have...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>swear god . dumb nigger bitch . got bleach hai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47690</th>\n",
       "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGE...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>yea fuck rt @ therealexel : your nigger fuck u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>bro . u got ta chill rt @ chillshrammi : dog f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47692 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type  \\\n",
       "0      In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1      Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2      @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3      @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4      @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "...                                                  ...                ...   \n",
       "47687  Black ppl aren't expected to do anything, depe...          ethnicity   \n",
       "47688  Turner did not withhold his disappointment. Tu...          ethnicity   \n",
       "47689  I swear to God. This dumb nigger bitch. I have...          ethnicity   \n",
       "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGE...          ethnicity   \n",
       "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...          ethnicity   \n",
       "\n",
       "                                       preprocessed_text  \n",
       "0             word # katandandr , food crapilici ! # mkr  \n",
       "1      # aussietv white ? # mkr # theblock # imaceleb...  \n",
       "2      @ xochitlsuckkk classi whore ? red velvet cupc...  \n",
       "3      @ jason_gio meh . : p thank head , concern ano...  \n",
       "4      @ rudhoeenglish isi account pretend kurdish ac...  \n",
       "...                                                  ...  \n",
       "47687  black ppl n't expect anyth , depend anyth . ye...  \n",
       "47688  turner withhold disappoint . turner call court...  \n",
       "47689  swear god . dumb nigger bitch . got bleach hai...  \n",
       "47690  yea fuck rt @ therealexel : your nigger fuck u...  \n",
       "47691  bro . u got ta chill rt @ chillshrammi : dog f...  \n",
       "\n",
       "[47692 rows x 3 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43acbab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               word # katandandr , food crapilici ! # mkr\n",
       "1        # aussietv white ? # mkr # theblock # imaceleb...\n",
       "2        @ xochitlsuckkk classi whore ? red velvet cupc...\n",
       "3        @ jason_gio meh . : p thank head , concern ano...\n",
       "4        @ rudhoeenglish isi account pretend kurdish ac...\n",
       "                               ...                        \n",
       "47687    black ppl n't expect anyth , depend anyth . ye...\n",
       "47688    turner withhold disappoint . turner call court...\n",
       "47689    swear god . dumb nigger bitch . got bleach hai...\n",
       "47690    yea fuck rt @ therealexel : your nigger fuck u...\n",
       "47691    bro . u got ta chill rt @ chillshrammi : dog f...\n",
       "Name: preprocessed_text, Length: 47692, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preprocessed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cabc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "This tweet is useless. I am going to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11db810",
   "metadata": {},
   "outputs": [],
   "source": [
    "['This', 'tweet','is','useless','.','I','am','going','to','delete','it','.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892be282",
   "metadata": {},
   "outputs": [],
   "source": [
    "['tweet', '.', 'useless', 'going', 'delete', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cddc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'tweet . useless go delete .'\n",
    "\n",
    "tweet useless go delete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5427e08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Matrix:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize a count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "count_matrix = vectorizer.fit_transform(df['preprocessed_text'])\n",
    "\n",
    "# Output vocabulary and count vector matrix\n",
    "print(\"Count Matrix:\\n\", count_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a6cbca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47692, 50935)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020800f",
   "metadata": {},
   "source": [
    "#### 4. Model Building\n",
    "\n",
    "Model building involves selecting a model (like Logistic Regression, Naive Bayes, etc.) and using the preprocessed and vectorized data to train this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f508958",
   "metadata": {},
   "source": [
    "**Example Code:** Training a Logistic Regression with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49f2be78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        not_cyberbullying\n",
       "1        not_cyberbullying\n",
       "2        not_cyberbullying\n",
       "3        not_cyberbullying\n",
       "4        not_cyberbullying\n",
       "               ...        \n",
       "47687            ethnicity\n",
       "47688            ethnicity\n",
       "47689            ethnicity\n",
       "47690            ethnicity\n",
       "47691            ethnicity\n",
       "Name: cyberbullying_type, Length: 47692, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cyberbullying_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b24475e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17357d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(raw_df, test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af482e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_df[inputs]\n",
    "train_target = train_df[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_inputs, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b535f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_target, test_target = train_test_split(count_matrix, labels, test_size=0.2,random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ma\n",
    "\n",
    "\n",
    "for m = 60, what is force? \n",
    "\n",
    "y = ma = 60*9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass of 78\n",
    "\n",
    "78*9.8 or 60*9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test set 90,65, 74, etc..\n",
    "\n",
    "90*9.8, 65*9.8, 74*9.8..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c69636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739af64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df\n",
    "\n",
    "\n",
    "train_df - the data on which model trains\n",
    "\n",
    "val_df - well - \n",
    "\n",
    "test_df -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0ab1f2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8274622573687994\n",
      "Accuracy: 0.8261112664243779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "labels = df['cyberbullying_type']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(count_matrix, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=54)\n",
    "\n",
    "# Initialize and train a Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation test and calculate accuracy \n",
    "val_preds = clf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ae9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_target,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181597d",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "This snippet demonstrates splitting the dataset into training and test sets, then initializing and training a Logistic Regression using the training data. After training, the model predicts labels for the test set. The accuracy of these predictions is then calculated against the true labels. Logistic Regression is a simple and effective baseline for text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9febd2",
   "metadata": {},
   "source": [
    "#### 5. Evaluation\n",
    "Finally, the model's performance is evaluated using metrics like accuracy, precision, recall, F1 score, depending on the specific task (classification, regression, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc482fff",
   "metadata": {},
   "source": [
    "**Example Code:** Continued from model building, adding precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "613e1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22d70efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.97079983 0.98786611 0.89968369 0.58445946 0.58227375 0.95869565]\n",
      "Recall: [0.97658578 0.9764268  0.84976526 0.56123277 0.65119197 0.94190517]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_test, predictions,average=None)\n",
    "recall = recall_score(y_test, predictions,average=None)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e580cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                age       0.97      0.98      0.97      2349\n",
      "          ethnicity       0.99      0.98      0.98      2418\n",
      "             gender       0.90      0.85      0.87      2343\n",
      "  not_cyberbullying       0.58      0.56      0.57      2466\n",
      "other_cyberbullying       0.58      0.65      0.61      2391\n",
      "           religion       0.96      0.94      0.95      2341\n",
      "\n",
      "           accuracy                           0.82     14308\n",
      "          macro avg       0.83      0.83      0.83     14308\n",
      "       weighted avg       0.83      0.82      0.83     14308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4846da",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "Following the model prediction, we calculate precision (the ratio of correctly predicted positive observations to the total predicted positives) and recall (the ratio of correctly predicted positive observations to the all observations in actual class). These metrics provide a more nuanced view of the model's performance than accuracy alone, especially in imbalanced datasets where positive and negative classes are not represented equally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
